{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mani4796/Coursera_project/blob/master/tutorials/asr/Voice_Activity_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12Yn6W1dt9t",
        "outputId": "58ab1907-df6b-4b75-adca-e6cc8d98ee45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=11c8440e01a99309a856f8baa1a94aea666560344d2c99093e0215fba743b043\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2build1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3 libwavpack1 sox\n",
            "0 upgraded, 7 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 617 kB of archives.\n",
            "After this operation, 1,764 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [104 kB]\n",
            "Fetched 617 kB in 0s (1,282 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 120876 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../2-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../3-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../4-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../6-sox_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (1.3)\n",
            "\u001b[33mDEPRECATION: git+https://github.com/NVIDIA/NeMo.git@r1.20.0#egg=nemo_toolkit[asr] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting nemo_toolkit[asr]\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision r1.20.0) to /tmp/pip-install-7380t8vn/nemo-toolkit_9d06a4d77a6c4ce0b7850f25182668c1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo.git /tmp/pip-install-7380t8vn/nemo-toolkit_9d06a4d77a6c4ce0b7850f25182668c1\n",
            "  Running command git checkout -b r1.20.0 --track origin/r1.20.0\n",
            "  Switched to a new branch 'r1.20.0'\n",
            "  Branch 'r1.20.0' set up to track remote branch 'r1.20.0' from 'origin'.\n",
            "  Resolved https://github.com/NVIDIA/NeMo.git to commit 2baef811f21372c3340dd2d82635d2377e78a660\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub (from nemo_toolkit[asr])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (0.56.4)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.23.5)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit[asr])\n",
            "  Downloading onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (2.8.2)\n",
            "Collecting ruamel.yaml (from nemo_toolkit[asr])\n",
            "  Downloading ruamel.yaml-0.17.35-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.2.2)\n",
            "Collecting setuptools==65.5.1 (from nemo_toolkit[asr])\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (2.13.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (4.66.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (3.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.15.0)\n",
            "Collecting braceexpand (from nemo_toolkit[asr])\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (0.6.2)\n",
            "Collecting g2p-en (from nemo_toolkit[asr])\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (7.7.1)\n",
            "Collecting jiwer (from nemo_toolkit[asr])\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Collecting kaldi-python-io (from nemo_toolkit[asr])\n",
            "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaldiio (from nemo_toolkit[asr])\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: librosa>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (0.10.1)\n",
            "Collecting marshmallow (from nemo_toolkit[asr])\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (23.2)\n",
            "Collecting pyannote.core (from nemo_toolkit[asr])\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.metrics (from nemo_toolkit[asr])\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from nemo_toolkit[asr])\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.11.3)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (0.12.1)\n",
            "Collecting sox (from nemo_toolkit[asr])\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting texterrors (from nemo_toolkit[asr])\n",
            "  Downloading texterrors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core<1.3,>=1.2.0 (from nemo_toolkit[asr])\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.3,>=2.2 (from nemo_toolkit[asr])\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning<=1.9.4,>=1.9.0 (from nemo_toolkit[asr])\n",
            "  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.8/827.8 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.11.0 (from nemo_toolkit[asr])\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.0.1 (from nemo_toolkit[asr])\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from nemo_toolkit[asr])\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset<=0.1.62,>=0.1.48 (from nemo_toolkit[asr])\n",
            "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (7.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.5.3)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]) (1.10.13)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit[asr])\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece<1.0.0 (from nemo_toolkit[asr])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting youtokentome>=1.0.5 (from nemo_toolkit[asr])\n",
            "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core<1.3,>=1.2.0->nemo_toolkit[asr])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (3.0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo_toolkit[asr]) (0.39.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.3,>=2.2->nemo_toolkit[asr]) (6.0.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[asr]) (3.20.3)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (2023.6.0)\n",
            "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr])\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]) (8.1.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[asr]) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->nemo_toolkit[asr]) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[asr]) (3.27.7)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[asr]) (17.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.1->nemo_toolkit[asr]) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers>=4.0.1->nemo_toolkit[asr])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.0.1->nemo_toolkit[asr])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p-en->nemo_toolkit[asr]) (3.8.1)\n",
            "Collecting distance>=0.1.3 (from g2p-en->nemo_toolkit[asr])\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]) (3.0.9)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer->nemo_toolkit[asr])\n",
            "  Downloading rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_toolkit[asr]) (2023.3.post1)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core->nemo_toolkit[asr]) (2.4.0)\n",
            "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit[asr])\n",
            "  Downloading pyannote.database-5.0.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from pyannote.metrics->nemo_toolkit[asr])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo_toolkit[asr]) (0.9.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit[asr])\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (3.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]) (0.41.2)\n",
            "Collecting pybind11 (from texterrors->nemo_toolkit[asr])\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plac (from texterrors->nemo_toolkit[asr])\n",
            "  Downloading plac-1.4.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting loguru (from texterrors->nemo_toolkit[asr])\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[asr]) (2.3.0)\n",
            "Collecting Levenshtein (from texterrors->nemo_toolkit[asr])\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->nemo_toolkit[asr])\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[asr]) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->nemo_toolkit[asr])\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->nemo_toolkit[asr])\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->nemo_toolkit[asr])\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->nemo_toolkit[asr])\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[asr]) (1.4.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit[asr]) (2.21)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (3.8.6)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr])\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[asr]) (1.3.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[asr]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[asr]) (6.3.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr])\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.9.0->nemo_toolkit[asr]) (3.11.0)\n",
            "Requirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->nemo_toolkit[asr]) (1.3.0)\n",
            "Collecting huggingface-hub (from nemo_toolkit[asr])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->nemo_toolkit[asr]) (2.1.3)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (6.5.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]) (1.3.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (0.8.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (5.4.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[asr]) (3.2.2)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr])\n",
            "  Downloading shellingham-1.5.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (13.6.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (4.19.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (3.0.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.10.6)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.6.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (0.1.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]) (1.1.3)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, sacremoses, youtokentome, kaldi-python-io, nemo_toolkit, distance, docopt, pathtools\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a68285bd4f9075b150b74db524574b60e015fbb048804fe8003558989696ea2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=67d34eb5beef1ea7a6cb1627565f88201d1a726cf90676c25965fab62ba6a006\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1951065 sha256=3798baa9cd3d3f20515c279a7b145c9bfed80ea42b02c0bbb8f31410770d0607\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n",
            "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8948 sha256=ee0f01dc3a8286b87111633849d259d84a735e208b0ad8e8a17b4ee35c137225\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/23/5f/49d3a826be576faf61d84e8028e1914bb36a5586ee2613b087\n",
            "  Building wheel for nemo_toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_toolkit: filename=nemo_toolkit-1.20.0-py3-none-any.whl size=2464994 sha256=38012d785c634b5dfab5cf0b135416b2dd1be5e659fd76b7299fbbbcab806610\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qnabjfm_/wheels/b5/34/43/cc084cf170a827b8e1e1a669ce1ffd243f3ca76cf086fb0ffe\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=2d1fd26156790f25ddb2bc468af7d6dfa793da13982c4e1c23a53632d886e808\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=bb00be5380ba95440a384de6f71f0ff6a7778d0168802f4dc4030669cf364967\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=4f7c641b96faa7ae4e13a20f01c61e8b9040941630316b09e06b13c278ce28bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built antlr4-python3-runtime sacremoses youtokentome kaldi-python-io nemo_toolkit distance docopt pathtools\n",
            "Installing collected packages: sentencepiece, pydub, plac, pathtools, docopt, distance, braceexpand, antlr4-python3-runtime, youtokentome, webdataset, sox, smmap, shellingham, setuptools, setproctitle, sentry-sdk, safetensors, sacremoses, ruamel.yaml.clib, rapidfuzz, pybind11, onnx, omegaconf, marshmallow, loguru, lightning-utilities, kaldiio, kaldi-python-io, jedi, docker-pycreds, colorama, ruamel.yaml, pyannote.core, Levenshtein, jiwer, hydra-core, huggingface-hub, gitdb, tokenizers, texterrors, GitPython, g2p-en, wandb, transformers, pyannote.database, pyannote.metrics, torchmetrics, pytorch-lightning, nemo_toolkit\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 Levenshtein-0.23.0 antlr4-python3-runtime-4.9.3 braceexpand-0.1.7 colorama-0.4.6 distance-0.1.3 docker-pycreds-0.4.0 docopt-0.6.2 g2p-en-2.1.0 gitdb-4.0.10 huggingface-hub-0.17.3 hydra-core-1.2.0 jedi-0.19.1 jiwer-3.0.3 kaldi-python-io-1.2.2 kaldiio-2.18.0 lightning-utilities-0.9.0 loguru-0.7.2 marshmallow-3.20.1 nemo_toolkit-1.20.0 omegaconf-2.2.3 onnx-1.14.1 pathtools-0.1.2 plac-1.4.0 pyannote.core-5.0.0 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pybind11-2.11.1 pydub-0.25.1 pytorch-lightning-1.9.4 rapidfuzz-3.4.0 ruamel.yaml-0.17.35 ruamel.yaml.clib-0.2.8 sacremoses-0.0.53 safetensors-0.4.0 sentencepiece-0.1.99 sentry-sdk-1.32.0 setproctitle-1.3.3 setuptools-65.5.1 shellingham-1.5.3 smmap-5.0.1 sox-1.4.1 texterrors-0.4.4 tokenizers-0.14.1 torchmetrics-1.2.0 transformers-4.34.1 wandb-0.15.12 webdataset-0.1.62 youtokentome-1.0.6\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\n",
        "\n",
        "NOTE: User is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "\n",
        "# ## Install NeMo\n",
        "BRANCH = 'r1.20.0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
        "\n",
        "## Install TorchAudio\n",
        "!pip install torchaudio>=0.13.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4bP-TxBtnQT"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This VAD tutorial is based on the MarbleNet model from paper \"[MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection](https://arxiv.org/abs/2010.13886)\", which is an modification and extension of [MatchboxNet](https://arxiv.org/abs/2004.08531).\n",
        "\n",
        "The notebook will follow the steps below:\n",
        "\n",
        " - Dataset preparation: Instruction of downloading datasets. And how to convert it to a format suitable for use with nemo_asr\n",
        " - Audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)\n",
        "\n",
        " - Data augmentation using SpecAugment \"[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)\" to increase number of data samples.\n",
        "\n",
        " - Develop a small Neural classification model which can be trained efficiently.\n",
        "\n",
        " - Model training on the Google Speech Commands dataset and Freesound dataset in NeMo.\n",
        "\n",
        " - Evaluation of error cases of the model by audibly hearing the samples\n",
        "\n",
        " - Add more evaluation metrics and transfer learning/fine tune\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I62_LJzc-p2b"
      },
      "outputs": [],
      "source": [
        "# Some utility imports\n",
        "import os\n",
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_M8wpkwd7d7"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "## Download the background data\n",
        "We suggest to use the background categories of [freesound](https://freesound.org/) dataset  as our non-speech/background data.\n",
        "We provide scripts for downloading and resampling it. Please have a look at Data Preparation part in NeMo docs. Note that downloading this dataset may takes hours.\n",
        "\n",
        "**NOTE:** Here, this tutorial serves as a demonstration on how to train and evaluate models for vad using NeMo. We avoid using freesound dataset, and use `_background_noise_` category in Google Speech Commands Dataset as non-speech/background data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqGZEJhKtnQU"
      },
      "source": [
        "## Download the speech data\n",
        "   \n",
        "We will use the open source Google Speech Commands Dataset (we will use V2 of the dataset for the tutorial, but require very minor changes to support V1 dataset) as our speech data. Google Speech Commands Dataset V2 will take roughly 6GB disk space. These scripts below will download the dataset and convert it to a format suitable for use with nemo_asr.\n",
        "\n",
        "\n",
        "**NOTE**: You may additionally pass `--test_size` or `--val_size` flag for splitting train val and test data.\n",
        "You may additionally pass `--window_length_in_sec` flag for indicating the segment/window length. Default is 0.63s.\n",
        "\n",
        "**NOTE**: You may additionally pass a `--rebalance_method='fixed|over|under'` at the end of the script to rebalance the class samples in the manifest.\n",
        "* 'fixed': Fixed number of samples for each class. For example, train 500, val 100, and test 200. (Change number in script if you want)\n",
        "* 'over': Oversampling rebalance method\n",
        "* 'under': Undersampling rebalance method\n",
        "\n",
        "**NOTE**: We only take a small subset of speech data for demonstration, if you want to use entire speech data. Don't forget to **delete `--demo`** and change rebalance method/number.  `_background_noise_` category only has **6** audio files. So we would like to generate more based on the audio files to enlarge our background training data. If you want to use your own background noise data, just change the `background_data_root` and **delete `--demo`**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIRTO_4VtnQV"
      },
      "outputs": [],
      "source": [
        "tmp = 'src'\n",
        "data_folder = 'data'\n",
        "if not os.path.exists(tmp):\n",
        "    os.makedirs(tmp)\n",
        "if not os.path.exists(data_folder):\n",
        "    os.makedirs(data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0oxGst3tnQV"
      },
      "outputs": [],
      "source": [
        "script = os.path.join(tmp, 'process_vad_data.py')\n",
        "if not os.path.exists(script):\n",
        "    !wget -P $tmp https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/dataset_processing/process_vad_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HBF3fBetnQV"
      },
      "outputs": [],
      "source": [
        "speech_data_root = os.path.join(data_folder, 'google_dataset_v2')\n",
        "background_data_root = os.path.join(data_folder, 'google_dataset_v2/google_speech_recognition_v2/_background_noise_')# your <resampled freesound data directory>\n",
        "out_dir = os.path.join(data_folder, 'manifest')\n",
        "if not os.path.exists(speech_data_root):\n",
        "    os.mkdir(speech_data_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzjnSHmQtnQW"
      },
      "outputs": [],
      "source": [
        "# This may take a few minutes\n",
        "!python $script \\\n",
        "    --out_dir={out_dir} \\\n",
        "    --speech_data_root={speech_data_root} \\\n",
        "    --background_data_root={background_data_root}\\\n",
        "    --log \\\n",
        "    --demo \\\n",
        "    --rebalance_method='fixed'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTsxp0nZ1zqo"
      },
      "source": [
        "## Preparing the manifest file\n",
        "\n",
        "Manifest files are the data structure used by NeMo to declare a few important details about the data :\n",
        "\n",
        "1) `audio_filepath`: Refers to the path to the raw audio file <br>\n",
        "2) `label`: The class label (speech or background) of this sample <br>\n",
        "3) `duration`: The length of the audio file, in seconds.<br>\n",
        "4) `offset`: The start of the segment, in seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytTFGVe0g9wk"
      },
      "outputs": [],
      "source": [
        "# change below if you don't have or don't want to use rebalanced data\n",
        "train_dataset = 'data/manifest/balanced_background_training_manifest.json,data/manifest/balanced_speech_training_manifest.json'\n",
        "val_dataset = 'data/manifest/background_validation_manifest.json,data/manifest/speech_validation_manifest.json'\n",
        "test_dataset = 'data/manifest/balanced_background_testing_manifest.json,data/manifest/balanced_speech_testing_manifest.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0SZy9SEhOBf"
      },
      "source": [
        "## Read a few rows of the manifest file\n",
        "\n",
        "Manifest files are the data structure used by NeMo to declare a few important details about the data :\n",
        "\n",
        "1) `audio_filepath`: Refers to the path to the raw audio file <br>\n",
        "2) `command`: The class label (or speech command) of this sample <br>\n",
        "3) `duration`: The length of the audio file, in seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIuC7EyutnQW"
      },
      "outputs": [],
      "source": [
        "sample_test_dataset =  test_dataset.split(',')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYBidCMIhKQV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!head -n 5 {sample_test_dataset}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64iFWPTMtnQX"
      },
      "source": [
        "# Training - Preparation\n",
        "\n",
        "We will be training a MarbleNet model from paper \"[MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection](https://arxiv.org/abs/2010.13886)\", evolved from [QuartzNet](https://arxiv.org/pdf/1910.10261.pdf) and [MatchboxNet](https://arxiv.org/abs/2004.08531) model. The benefit of QuartzNet over JASPER models is that they use Separable Convolutions, which greatly reduce the number of parameters required to get good model accuracy.\n",
        "\n",
        "MarbleNet models generally follow the model definition pattern QuartzNet-[BxRXC], where B is the number of blocks, R is the number of convolutional sub-blocks, and C is the number of channels in these blocks. Each sub-block contains a 1-D masked convolution, batch normalization, ReLU, and dropout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieAPOM9thTN2"
      },
      "outputs": [],
      "source": [
        "# NeMo's \"core\" package\n",
        "import nemo\n",
        "# NeMo's ASR collection - this collections contains complete ASR models and\n",
        "# building blocks (modules) for ASR\n",
        "import nemo.collections.asr as nemo_asr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9gLcDv30jI"
      },
      "source": [
        "## Model Configuration\n",
        "The MarbleNet Model is defined in a config file which declares multiple important sections.\n",
        "\n",
        "They are:\n",
        "\n",
        "1) `model`: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets and any other related information\n",
        "\n",
        "2) `trainer`: Any argument to be passed to PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUt8MPBXtnQX"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG = \"marblenet_3x2x64.yaml\"\n",
        "\n",
        "if not os.path.exists(f\"configs/{MODEL_CONFIG}\"):\n",
        "  !wget -P configs/ \"https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/marblenet/{MODEL_CONFIG}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoVAs9h1lfci",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# This line will print the entire config of the MarbleNet model\n",
        "config_path = f\"configs/{MODEL_CONFIG}\"\n",
        "config = OmegaConf.load(config_path)\n",
        "config = OmegaConf.to_container(config, resolve=True)\n",
        "config = OmegaConf.create(config)\n",
        "\n",
        "print(OmegaConf.to_yaml(config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2lJPR0a3qww"
      },
      "outputs": [],
      "source": [
        "# Preserve some useful parameters\n",
        "labels = config.model.labels\n",
        "sample_rate = config.model.sample_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_pmjeed78rJ"
      },
      "source": [
        "### Setting up the datasets within the config\n",
        "\n",
        "If you'll notice, there are a few config dictionaries called `train_ds`, `validation_ds` and `test_ds`. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIe6Qfs18MiQ"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(config.model.train_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb01hl868Uc3"
      },
      "source": [
        "### `???` inside configs\n",
        "\n",
        "You will often notice that some configs have `???` in place of paths. This is used as a placeholder so that the user can change the value at a later time.\n",
        "\n",
        "Let's add the paths to the manifests to the config above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m181HXev8T97"
      },
      "outputs": [],
      "source": [
        "config.model.train_ds.manifest_filepath = train_dataset\n",
        "config.model.validation_ds.manifest_filepath = val_dataset\n",
        "config.model.test_ds.manifest_filepath = test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbXngoCM5IRG"
      },
      "source": [
        "## Building the PyTorch Lightning Trainer\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
        "\n",
        "Let's first instantiate a Trainer object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYtvdBlG5afU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRN18CdH51nN"
      },
      "outputs": [],
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHf6cHvm6H9b"
      },
      "outputs": [],
      "source": [
        "# Let's modify some trainer configs for this demo\n",
        "# Checks if we have GPU available and uses it\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "config.trainer.devices = 1\n",
        "config.trainer.accelerator = accelerator\n",
        "\n",
        "# Reduces maximum number of epochs to 5 for quick demonstration\n",
        "config.trainer.max_epochs = 5\n",
        "\n",
        "# Remove distributed training flags\n",
        "config.trainer.strategy = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB9nr7G56G3L"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(**config.trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wt603Vq6sqX"
      },
      "source": [
        "## Setting up a NeMo Experiment\n",
        "\n",
        "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfWJFg7p6Ezf"
      },
      "outputs": [],
      "source": [
        "from nemo.utils.exp_manager import exp_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC-QPoW44-p2"
      },
      "outputs": [],
      "source": [
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqi6rkNR7Dph"
      },
      "outputs": [],
      "source": [
        "# The exp_dir provides a path to the current experiment for easy access\n",
        "exp_dir = str(exp_dir)\n",
        "exp_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0zz-vHH7Uuh"
      },
      "source": [
        "## Building the MarbleNet Model\n",
        "\n",
        "MarbleNet is an ASR model with a classification task - it generates one label for the entire provided audio stream. Therefore we encapsulate it inside the `EncDecClassificationModel` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRMrKhyf5vhy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "vad_model = nemo_asr.models.EncDecClassificationModel(cfg=config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA9UND-Q_oyw"
      },
      "source": [
        "# Training a MarbleNet Model\n",
        "\n",
        "As MarbleNet is inherently a PyTorch Lightning Model, it can easily be trained in a single line - `trainer.fit(model)` !\n",
        "\n",
        "\n",
        "# Training the model\n",
        "\n",
        "Even with such a small model (73k parameters), and just 5 epochs (should take just a few minutes to train), you should be able to get a test set accuracy score around 98.83% (this result is for the [freesound](https://freesound.org/) dataset) with enough training data.\n",
        "\n",
        "**NOTE:** If you follow our tutorial and user the generated background data, you may notice the below results are acceptable, but please remember, this tutorial is only for **demonstration** and the dataset is not good enough. Please change background dataset and train with enough data for improvement!\n",
        "\n",
        "Experiment with increasing the number of epochs or with batch size to see how much you can improve the score!\n",
        "\n",
        "**NOTE:** Noise robustness is quite important for VAD task. Below we list the augmentation we used in this demo.\n",
        "Please refer to [Online_Noise_Augmentation.ipynb](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_Noise_Augmentation.ipynb)  for understanding noise augmentation in NeMo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsPcOfbctnQZ"
      },
      "outputs": [],
      "source": [
        "# Noise augmentation\n",
        "print(OmegaConf.to_yaml(config.model.train_ds.augmentor)) # noise augmentation\n",
        "print(OmegaConf.to_yaml(config.model.spec_augment)) # SpecAug data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5u-abqYtnQZ"
      },
      "source": [
        "If you are interested in  **pretrained** model, please have a look at [Transfer Leaning & Fine-tuning on a new dataset](#Transfer-Leaning-&-Fine-tuning-on-a-new-dataset) and incoming tutorial 07 Offline_and_Online_VAD_Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ngKcRFqBfIF"
      },
      "source": [
        "### Monitoring training progress\n",
        "\n",
        "Before we begin training, let's first create a Tensorboard visualization to monitor progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyfec0PDBsXa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google import colab\n",
        "    COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if COLAB_ENV:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir {exp_dir}\n",
        "else:\n",
        "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZApuELDIKQgC"
      },
      "source": [
        "### Training for 5 epochs\n",
        "We see below that the model begins to get modest scores on the validation set after just 5 epochs of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xiUUJlH5KdD"
      },
      "outputs": [],
      "source": [
        "trainer.fit(vad_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaPdSzpWtnQc"
      },
      "source": [
        "# Fast Training\n",
        "\n",
        "We can dramatically improve the time taken to train this model by using Multi GPU training along with Mixed Precision.\n",
        "\n",
        "```python\n",
        "# Trainer with a distributed backend:\n",
        "trainer = Trainer(devices=2, num_nodes=2, accelerator='gpu', strategy='dp')\n",
        "\n",
        "# Mixed precision:\n",
        "trainer = Trainer(amp_level='O1', precision=16)\n",
        "\n",
        "# Of course, you can combine these flags as well.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkds1jSvKgSc"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "## Evaluation on the Test set\n",
        "\n",
        "Let's compute the final score on the test set via `trainer.test(model)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULTrhEJ_6wV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "trainer.test(vad_model, ckpt_path=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDHkunjM8y6"
      },
      "source": [
        "## Evaluation of incorrectly predicted samples\n",
        "\n",
        "Given that we have a trained model, which performs reasonably well, let's try to listen to the samples where the model is least confident in its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJrZ72sNCkM"
      },
      "source": [
        "### Extract the predictions from the model\n",
        "\n",
        "We want to possess the actual logits of the model instead of just the final evaluation score, so we can define a function to perform the forward step for us without computing the final loss. Instead, we extract the logits per batch of samples provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxdviYtOFjK"
      },
      "source": [
        "### Accessing the data loaders\n",
        "\n",
        "We can utilize the `setup_test_data` method in order to instantiate a data loader for the dataset we want to analyze.\n",
        "\n",
        "For convenience, we can access these instantiated data loaders using the following accessors - `vad_model._train_dl`, `vad_model._validation_dl` and `vad_model._test_dl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB0QZCAmM656"
      },
      "outputs": [],
      "source": [
        "vad_model.setup_test_data(config.model.test_ds)\n",
        "test_dl = vad_model._test_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA7gXawcPoip"
      },
      "source": [
        "### Partial Test Step\n",
        "\n",
        "Below we define a utility function to perform most of the test step. For reference, the test step is defined as follows:\n",
        "\n",
        "```python\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        audio_signal, audio_signal_len, labels, labels_len = batch\n",
        "        logits = self.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
        "        loss_value = self.loss(logits=logits, labels=labels)\n",
        "        correct_counts, total_counts = self._accuracy(logits=logits, labels=labels)\n",
        "        return {'test_loss': loss_value, 'test_correct_counts': correct_counts, 'test_total_counts': total_counts}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBsDOm5ROpQI"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def extract_logits(model, dataloader):\n",
        "    logits_buffer = []\n",
        "    label_buffer = []\n",
        "\n",
        "    # Follow the above definition of the test_step\n",
        "    for batch in dataloader:\n",
        "        audio_signal, audio_signal_len, labels, labels_len = batch\n",
        "        logits = model(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
        "\n",
        "        logits_buffer.append(logits)\n",
        "        label_buffer.append(labels)\n",
        "        print(\".\", end='')\n",
        "    print()\n",
        "\n",
        "    print(\"Finished extracting logits !\")\n",
        "    logits = torch.cat(logits_buffer, 0)\n",
        "    labels = torch.cat(label_buffer, 0)\n",
        "    return logits, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZSdprUlOuoV"
      },
      "outputs": [],
      "source": [
        "cpu_model = vad_model.cpu()\n",
        "cpu_model.eval()\n",
        "logits, labels = extract_logits(cpu_model, test_dl)\n",
        "print(\"Logits:\", logits.shape, \"Labels :\", labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wd0ukgNXRBz",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Compute accuracy - `_accuracy` is a PyTorch Lightning Metric !\n",
        "acc = cpu_model._accuracy(logits=logits, labels=labels)\n",
        "print(f\"Accuracy : {float(acc[0]*100)} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN9OSqCauSH"
      },
      "source": [
        "### Filtering out incorrect samples\n",
        "Let us now filter out the incorrectly labeled samples from the total set of samples in the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1YJvsmcZ0uE"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import json\n",
        "import IPython.display as ipd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZAT9yGAayvR"
      },
      "outputs": [],
      "source": [
        "# First let's create a utility class to remap the integer class labels to actual string label\n",
        "class ReverseMapLabel:\n",
        "    def __init__(self, data_loader):\n",
        "        self.label2id = dict(data_loader.dataset.label2id)\n",
        "        self.id2label = dict(data_loader.dataset.id2label)\n",
        "\n",
        "    def __call__(self, pred_idx, label_idx):\n",
        "        return self.id2label[pred_idx], self.id2label[label_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3GSXvYHa4KJ"
      },
      "outputs": [],
      "source": [
        "# Next, let's get the indices of all the incorrectly labeled samples\n",
        "sample_idx = 0\n",
        "incorrect_preds = []\n",
        "rev_map = ReverseMapLabel(test_dl)\n",
        "\n",
        "# Remember, evaluated_tensor = (loss, logits, labels)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "probas, preds = torch.max(probs, dim=-1)\n",
        "\n",
        "total_count = cpu_model._accuracy.total_counts_k[0]\n",
        "incorrect_ids = (preds != labels).nonzero()\n",
        "for idx in incorrect_ids:\n",
        "    proba = float(probas[idx][0])\n",
        "    pred = int(preds[idx][0])\n",
        "    label = int(labels[idx][0])\n",
        "    idx = int(idx[0]) + sample_idx\n",
        "\n",
        "    incorrect_preds.append((idx, *rev_map(pred, label), proba))\n",
        "\n",
        "\n",
        "print(f\"Num test samples : {total_count.item()}\")\n",
        "print(f\"Num errors : {len(incorrect_preds)}\")\n",
        "\n",
        "# First let's sort by confidence of prediction\n",
        "incorrect_preds = sorted(incorrect_preds, key=lambda x: x[-1], reverse=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JgGo71gcDtD"
      },
      "source": [
        "### Examine a subset of incorrect samples\n",
        "Let's print out the (test id, predicted label, ground truth label, confidence) tuple of first 20 incorrectly labeled samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x37wNJsNbcw0"
      },
      "outputs": [],
      "source": [
        "for incorrect_sample in incorrect_preds[:20]:\n",
        "    print(str(incorrect_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDnwYsDKcLv9"
      },
      "source": [
        "###  Define a threshold below which we designate a model's prediction as \"low confidence\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpvzeh4PcGJs"
      },
      "outputs": [],
      "source": [
        "# Filter out how many such samples exist\n",
        "low_confidence_threshold = 0.8\n",
        "count_low_confidence = len(list(filter(lambda x: x[-1] <= low_confidence_threshold, incorrect_preds)))\n",
        "print(f\"Number of low confidence predictions : {count_low_confidence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERXyXvCAcSKR"
      },
      "source": [
        "### Let's hear the samples which the model has least confidence in !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxjNVjX8cPNP"
      },
      "outputs": [],
      "source": [
        "# First let's create a helper function to parse the manifest files\n",
        "def parse_manifest(manifest):\n",
        "    data = []\n",
        "    for line in manifest:\n",
        "        line = json.loads(line)\n",
        "        data.append(line)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWxqw5k-cUVd"
      },
      "outputs": [],
      "source": [
        "# Next, let's create a helper function to actually listen to certain samples\n",
        "def listen_to_file(sample_id, pred=None, label=None, proba=None):\n",
        "    # Load the audio waveform using librosa\n",
        "    filepath = test_samples[sample_id]['audio_filepath']\n",
        "    audio, sample_rate = librosa.load(filepath,\n",
        "                                      offset = test_samples[sample_id]['offset'],\n",
        "                                      duration = test_samples[sample_id]['duration'])\n",
        "\n",
        "\n",
        "    if pred is not None and label is not None and proba is not None:\n",
        "        print(f\"filepath: {filepath}, Sample : {sample_id} Prediction : {pred} Label : {label} Confidence = {proba: 0.4f}\")\n",
        "    else:\n",
        "\n",
        "        print(f\"Sample : {sample_id}\")\n",
        "\n",
        "    return ipd.Audio(audio, rate=sample_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPj1tFNIcXaU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# Now let's load the test manifest into memory\n",
        "all_test_samples = []\n",
        "for _ in test_dataset.split(','):\n",
        "    print(_)\n",
        "    with open(_, 'r') as test_f:\n",
        "        test_samples = test_f.readlines()\n",
        "\n",
        "        all_test_samples.extend(test_samples)\n",
        "print(len(all_test_samples))\n",
        "test_samples = parse_manifest(all_test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt7b_uiScZcC"
      },
      "outputs": [],
      "source": [
        "# Finally, let's listen to all the audio samples where the model made a mistake\n",
        "# Note: This list of incorrect samples may be quite large, so you may choose to subsample `incorrect_preds`\n",
        "count = min(count_low_confidence, 20)  # replace this line with just `count_low_confidence` to listen to all samples with low confidence\n",
        "\n",
        "for sample_id, pred, label, proba in incorrect_preds[:count]:\n",
        "    ipd.display(listen_to_file(sample_id, pred=pred, label=label, proba=proba))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtOGvVcOtnQf"
      },
      "source": [
        "## Adding evaluation metrics\n",
        "\n",
        "Here is an example of how to use more metrics (e.g. from torchmetrics) to evaluate your result.\n",
        "\n",
        "**Note:** If you would like to add metrics for training and testing, have a look at\n",
        "```python\n",
        "NeMo/nemo/collections/common/metrics\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IyrM9qjtnQf"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import ConfusionMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2BPOhAOtnQf"
      },
      "outputs": [],
      "source": [
        "_, pred = logits.topk(1, dim=1, largest=True, sorted=True)\n",
        "pred = pred.squeeze()\n",
        "metric = ConfusionMatrix(num_classes=2, task='binary')\n",
        "metric(pred, labels)\n",
        "# confusion_matrix(preds=pred, target=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sQ1rQX8tnQg"
      },
      "source": [
        "# Transfer Leaning & Fine-tuning on a new dataset\n",
        "For transfer learning, please refer to [**Transfer learning** part of ASR tutorial](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_NeMo.ipynb)\n",
        "\n",
        "More details on saving and restoring checkpoint, and exporting a model in its entirety, please refer to [**Fine-tuning on a new dataset** & **Advanced Usage parts** of Speech Command tutorial](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Speech_Commands.ipynb)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyIegk2CPNsI"
      },
      "source": [
        "# Inference and more\n",
        "If you are interested in **pretrained** model and **streaming inference**, please have a look at our [VAD inference tutorial](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_Offline_Microphone_VAD_Demo.ipynb) and script [vad_infer.py](https://github.com/NVIDIA/NeMo/blob/stable/examples/asr/speech_classification/vad_infer.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkqPzn4rtnQg"
      },
      "source": [
        "# Frame-VAD: More Effective and Efficient VAD for More Fine-grained Timestamps\n",
        "\n",
        "In this notebook, we are using the segment-VAD model, which predicts a single label for each short segment (0.63s), which is not optimal for some applications that require very precise timestamps.\n",
        "\n",
        "To get more precise timestamps, we can use a frame-VAD model, which predicts a label for each input frame (20ms). To prepare manifest for frame-VAD, you'll need to have `label` field in each manifest entry, which is a string of labels for each frame. For example, if you have a 1s audio file, you'll need to have 50 frame labels in the manifest entry like \"0 0 0 0 1 1 0 1 .... 0 1\".\n",
        "However, shorter label strings are also supported for smaller file sizes. For example, you can prepare the `label` in 40ms frame, and the model will properly repeat the label for each 20ms frame.\n",
        "\n",
        "The Frame-VAD model shares the same MarbleNet architecture as the segment-VAD model, but with a different input/output resolution and loss function. The frame-VAD model is trained with more data than segment-VAD and achieves better performance, as shown in the [NGC model card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/vad_multilingual_frame_marblenet).\n",
        "\n",
        "During inference, since frame-VAD model doesn't require splicing input into overlapping segments, it is more efficient than segment-VAD model, with 8x less GPU memory consumption.\n",
        "\n",
        "For more information on the frame-VAD model, please refer to the [README.md](https://github.com/NVIDIA/NeMo/blob/stable/examples/asr/speech_classification/README.md). For training and running inference on frame-VAD, please refer to [speech_to_frame_label.py](https://github.com/NVIDIA/NeMo/blob/stable/examples/asr/speech_classification/speech_to_frame_label.py) and [frame_vad_infer.py](https://github.com/NVIDIA/NeMo/blob/stable/examples/asr/speech_classification/frame_vad_infer.py)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Voice_Activity_Detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}